"""ChIPseq pipeline

Affiliation: Institut Pasteur @ 2018

This pipeline is part of Sequana software (sequana.readthedocs.io)
"""

import sequana
from os.path import join
from sequana import snaketools as sm
import pandas as pd
from fnmatch import fnmatch
from re import sub
from itertools import repeat

sm.init("chipseq.rules", globals())

# This must be defined before the include
configfile: "config.yaml"
__snakefile__ = srcdir(__snakefile__)

# Generic include of some dynamic modules
exec(open(sequana.modules["fastqc_dynamic"], "r").read())
exec(open(sequana.modules["bowtie2_mapping_dynamic"], "r").read())
exec(open(sequana.modules["bowtie2_index_dynamic"], "r").read())
exec(open(sequana.modules["dynamic_unpigz"], "r").read())
exec(open(sequana.modules["macs2_dynamic"], "r").read())
exec(open(sequana.modules["preIDR_dynamic"], "r").read())


manager = sm.PipelineManager("chipseq", config)



# Check the design file
__design__ = manager.config.design.design_file

design = pd.read_csv(__design__, header=0, sep='\t')



"""
REQUIREMENTS IN DESIGN:
 - all files in one directory
 - fullname of files must be :
        MARK_COND_REP_READ-TAD.fastq.gz
 - name on design must be:
        MARK_COND


"""

#get all fastq
__data__input = manager.getrawdata()


#get list of input files
INPUT = []
for row in design.itertuples(index=True, name='Pandas'):
    for file in manager.ff.filenames:
        mark = getattr(row, "INPUT_NAME")
        if fnmatch(file, mark+"*1_*"):
            i = 1
            name = sub(manager.ff.read_tag, '', file)
            INPUT.append(name)
            if getattr(row, "NB_IP") > 1 :
                while i < getattr(row, "NB_IP"):
                    INPUT.append(name)
                    i += 1

#get list of IP files
IP_ALL = []
for mark in design['IP_NAME']:
    for file in manager.ff.filenames:
        if fnmatch(file, mark+"*") and file not in IP_ALL:
            name = sub(manager.ff.read_tag, '', file)
            IP_ALL.append(name)


# get REP names
#get list with replicates names for all IP
rep_flag = manager.config.design.replicates
rep = ['Rep1', 'Rep2']
# deduce from rep the list for SPR & PPR
ppr = ['PPR1', 'PPR2', 'PPRPool']
spr = ['SPR1.1', 'SPR1.2', 'SPR2.1', 'SPR2.2']

#check design
# get marks and conds for the union of optimal peaks
marks = [ x.strip() for x in (manager.config.design.marks).split(",")]
conds = [ x.strip() for x in (manager.config.design.condition).split(",")]

#built list of MARK_COND_REP from config.yaml and check correspondance between design, config and fastq files

if len(conds) > 1 :
#built list of MARK_COND_REP from config.yaml
    conf_cond = ["{mark}_{cond}_{flag}".format(cond=cond, mark=mark, flag=rep_flag) for cond in conds for mark in marks]
    for row in design.itertuples(index=True, name='Pandas'):
        for elem in conf_cond:
            if elem in getattr(row, "IP_NAME"):
                raise ValueError("Please check correspondance between config and design file: %s is not %s "
                                 % (elem,getattr(row, "IP_NAME")))
            elif not any(elem in s for s in manager.ff.tags):
                raise ValueError("Please check correspondance between config file and fastq filenames: %s not found in %s"
                                 % (elem, manager.ff.tags))
            elif sum(getattr(row, "IP_NAME") in s for s in manager.ff.tags) is not int(getattr(row, "NB_IP")):
                raise ValueError("Please check correspondance between number of replicates and/or prefix names in design "
                                 "file and fastq filenames: %s not found %s times in %s" % (getattr(row, "IP_NAME"),
                                                                                             getattr(row, "NB_IP"),manager.ff.tags))
else :
    pass

# From the design file, we get only IP with more than one replicate
IP_REP = [] # all IP passing IDR
IP_REP_DUP = [] # corresponding INPUT
for row in design.itertuples(index=True, name='Pandas'):
    if getattr(row, "NB_IP") > 1:
        IP_REP_DUP.append(getattr(row, "INPUT_NAME")+"_"+rep_flag+"1")
        IP_REP.append(getattr(row, "IP_NAME"))

# We get only INPUT with more than one replicate that are linked to an IP with multiple replicates
INPUT_REP = []
for row in design.itertuples(index=True, name='Pandas'):
    if getattr(row, "NB_INPUT") > 1 and getattr(row, "NB_IP") > 1 and (getattr(row, "INPUT_NAME")) not in INPUT_REP:
        INPUT_REP.append(getattr(row, "INPUT_NAME"))

#get IP passed pre IDR step (for rep, ppr and spr)
IP_IDR = []
IP_SPR = []
IP_PPR = []
SPR_POOL = []
INPUT_SPR  = []
INPUT_PPR = []
#IDR_REP = []
for cond in IP_REP:
    tmp = []
    tmp2 = []
    for ip in IP_ALL:
        name = ip.split("_" + rep_flag)
        if ip.startswith(cond):
            spr_file = sub(rep_flag, 'SPR', ip)
            ppr_file =  sub(rep_flag, 'PPR', ip)
            pool_file = sub(rep_flag, 'PPRPool', ip)
            tmp.append(ip)
            #IDR_REP.append("Rep"+name[1])
            IP_SPR.append(spr_file+".1")
            IP_SPR.append(spr_file+".2")
            IP_PPR.append(ppr_file)
            SPR_POOL.append(pool_file)
            for row in design.itertuples(index=True, name='Pandas'):
                if  (getattr(row, "IP_NAME")) in name[0]:
                     INPUT_PPR.append(getattr(row, "INPUT_NAME")+"_"+rep[0])
                     INPUT_SPR.append(getattr(row, "INPUT_NAME")+"_"+rep[0])
                     INPUT_SPR.append(getattr(row, "INPUT_NAME")+"_"+rep[0])
    IP_IDR.append(tmp)


#get pooled IP passed pre IDR step and corresponding INPUT
PPR_POOL = []
INPUT_POOL = []
for row in design.itertuples(index=True, name='Pandas'):
    if getattr(row, "NB_IP") > 1 and getattr(row, "NB_INPUT") > 1 :
        PPR_POOL.append(getattr(row, "IP_NAME") + "_PPRPool")
        INPUT_POOL.append(getattr(row, "INPUT_NAME") + "_Pool")
    elif getattr(row, "NB_IP") > 1 and getattr(row, "NB_INPUT") < 2 :
        PPR_POOL.append(getattr(row, "IP_NAME") + "_PPRPool")
        INPUT_POOL.append(getattr(row, "INPUT_NAME")+ "_Rep1")

# all files passing PhantomPeakQualTool if no-model is chosen
ALL = IP_ALL + IP_SPR + IP_PPR + PPR_POOL

# get files for IDR
CASE = ["Rep", "PPR", "SPR1.", "SPR2."]*len(IP_REP)
REP_IDR = list(chain(*zip(*repeat(IP_REP,4))))
IN_IDR = list(chain(*zip(*repeat(IP_REP_DUP,4))))


# Add wildcard constraints
wildcard_constraints:
    sample = "[A-Za-z-_0-9]+_{0}[0-9]+".format(rep_flag),
    IP_REP = "[A-Za-z-_0-9]+_{0}[0-9]+".format(rep_flag),
    REP = "{0}[0-9]+".format(rep_flag),
    SPR = "[A-Za-z-_0-9]+_SPR[0-9]\.[1-4]*",
    PPR = "[A-Za-z-_0-9]+_PPR[0-9]*",
    POOL = "[A-Za-z-_0-9]+_PPRPool",
    INPUT_POOL = "[A-Za-z-_0-9]+_(Pool|{0}1)".format(rep_flag),
    MARK = "[A-Za-z-_0-9]+"

#reorder rules :
ruleorder: mark_duplicates > spp > preIDR_input > preIDR_PPR > preIDR_SPR > macs2_rep > compute_idr

if manager.config.fastqc.do:
    # FASTQC on input data set
    __fastqc_raw__input_fastq = __data__input
    __fastqc_raw__output_done = "0-Fastqc/{sample}_fastqc_raw.done"
    __fastqc_raw__wkdir = "0-Fastqc"
    __fastqc_raw__log = "0-Fastqc/logs/{sample}_fastqc_raw.log"
    include: fastqc_dynamic("raw", manager)
    expected_output.extend(expand(__fastqc_raw__output_done, sample=manager.samples))

if manager.config.cutadapt.do:
    adapter_tool = manager.config.cutadapt.tool_choice

    from sequana.adapters import _get_registered_adapters as registered
    from sequana.adapters import get_sequana_adapters

    # Users may provide TruSeq, Nextera, PCRFree or other registered adapters
    fwd = manager.config.cutadapt.fwd
    if isinstance(fwd, str) and fwd in registered():
        filename = "file:" + get_sequana_adapters(fwd, "fwd")
        manager.config.cutadapt.fwd = filename

    rev = manager.config.cutadapt.rev
    if isinstance(rev, str) and rev in registered():
        filename = "file:" + get_sequana_adapters(rev, "revcomp")
        manager.config.cutadapt.rev = filename

    if adapter_tool in ["cutadapt", "atropos"]:
        adapter_tool = "cutadapt"
        __cutadapt__input_fastq = __data__input
        __cutadapt__wkdir = "1-Trimming"
        # __cutadapt__output = [manager.getname("cutadapt", "_R1_.cutadapt.fastq.gz")]
        __cutadapt__output = ["1-Trimming/{sample}_R1_trim.fastq.gz"]
        if manager.paired:
            __cutadapt__output += ["1-Trimming/{sample}_R2_trim.fastq.gz"]

        # Set the fwd and rev adapters
        __cutadapt__fwd = manager.config.cutadapt.fwd
        __cutadapt__rev = manager.config.cutadapt.rev

        __cutadapt__design = manager.config.cutadapt.design_file
        __cutadapt__design_adapter = manager.config['cutadapt']['adapter_choice']
        __cutadapt__options = manager.config.cutadapt.options
        __cutadapt__mode = manager.config.cutadapt.mode
        __cutadapt__log = "1-Trimming/logs/{sample}_trim.txt"
        __cutadapt__sample = "{sample}"
        include: sm.modules["cutadapt"]

    else:
        raise ValueError("Invalid choice of cutadapt:tool in config file. Use either atropos or cutadapt")
else:
    __cutadapt__output = __data__input

if manager.config.cutadapt.do:
    # FASTQC on trimmed data set
    __fastqc_trim__input_fastq = __cutadapt__output
    __fastqc_trim__output_done = "0-Fastqc/{sample}_fastqc_trim.done"
    __fastqc_trim__wkdir = "0-Fastqc"
    __fastqc_trim__log = "0-Fastqc/logs/{sample}_fastqc_trim.log"
    include: fastqc_dynamic("trim", manager)
    expected_output.extend(expand(__fastqc_trim__output_done, sample=manager.samples))

__prefix_name__ = os.path.join(config["genome"]["genome_directory"], config["genome"]["name"])

if manager.config.genome.do:
    # indexing for bowtie2
    __bowtie2_index_ref__fasta = config["genome"]["fasta_file"]
    __bowtie2_index_ref__output_done = __prefix_name__ + ".1.bt2"
    __bowtie2_index_ref__output_prefix = __prefix_name__
    __bowtie2_index_ref__log = "2-Mapping/logs/bowtie2_ref_indexing.log"
    include: bowtie2_index_dynamic("ref")
    expected_output.extend([__bowtie2_index_ref__output_done])
else:
    __bowtie2_index_ref__output_done = __prefix_name__ + ".1.bt2"
    __bowtie2_index_ref__output_prefix = __prefix_name__

ref = manager.config.genome.name

if manager.config.bowtie2_mapping.do:
    # Decompress fastq.gz file before to run bowtie2 ### NECESSARY ??
    __unpigz_R1__input = "1-Trimming/{sample}_R1_trim.fastq.gz"
    __unpigz_R1__output = "1-Trimming/{sample}_R1_trim.fastq"
    include: dynamic_unpigz("R1", manager)
    __unpigz__output = [__unpigz_R1__output]
    if manager.paired:
        __unpigz_R2__input = "1-Trimming/{sample}_R2_trim.fastq.gz"
        __unpigz_R2__output = "1-Trimming/{sample}_R2_trim.fastq"
        include: dynamic_unpigz("R2", manager)
        __unpigz__output += [__unpigz_R2__output]


    # mapping on ref
    __bowtie2_mapping_ref__input = __cutadapt__output
    __bowtie2_mapping_ref__index_done = __bowtie2_index_ref__output_done
    __bowtie2_mapping_ref__sort = "2-Mapping/{{sample}}_{}_sort.bam".format(ref)
    __bowtie2_mapping_ref__bam = "2-Mapping/{{sample}}_{}.bam".format(ref)
    __bowtie2_mapping_ref__logs_err = "2-Mapping/logs/{{sample}}_{}_mapping.e".format(ref)
    __bowtie2_mapping_ref__logs_out = "2-Mapping/logs/{{sample}}_{}_mapping.o".format(ref)
    __bowtie2_mapping_ref__prefix_index = __bowtie2_index_ref__output_prefix
    include: bowtie2_mapping_dynamic("ref", manager)
    expected_output.extend(expand(__bowtie2_mapping_ref__sort, sample=manager.samples))

else:
    __bowtie2_mapping_ref__sort = __data__input

if manager.config.design.spike:
    # indexing for bowtie2
    __spikes_fasta = config["design"]["spike_genome_file"]
    __bowtie2_index_spike__fasta = __spikes_fasta
    __bowtie2_index_spike__output_done = os.path.splitext(__spikes_fasta)[0] + ".1.bt2"
    __bowtie2_index_spike__output_prefix = os.path.splitext(__spikes_fasta)[0]
    __bowtie2_index_spike__log = "2-Mapping/logs/bowtie2_spike_indexing.log"
    include: bowtie2_index_dynamic("spike")
    expected_output.extend([__bowtie2_index_spike__output_done])

    # mapping on spike
    __bowtie2_mapping_spike__input = __cutadapt__output
    __bowtie2_mapping_spike__index_done = __bowtie2_index_spike__output_done
    __bowtie2_mapping_spike__sort = "2-Mapping/{sample}_spike_sort.bam"
    __bowtie2_mapping_spike__bam = "2-Mapping/{sample}_spike.bam"
    __bowtie2_mapping_spike__logs_err = "2-Mapping/logs/{sample}_spike_mapping.e"
    __bowtie2_mapping_spike__logs_out = "2-Mapping/logs/{sample}_spike_mapping.o"
    __bowtie2_mapping_spike__prefix_index = __bowtie2_index_spike__output_prefix
    include: bowtie2_mapping_dynamic("spike", manager)
    expected_output.extend(expand(__bowtie2_mapping_spike__sort, sample=manager.samples))

    # counting on spikes
    __spikes_counting__input = expand("2-Mapping/{sample}_spike.bam", sample=manager.samples)
    __spikes_counting__output = "2-Mapping/ALL_count_on_spike.mtx"
    __spikes_counting__output_json = "2-Mapping/ALL_count_on_spike.json"
    __spikes_counting__log = "2-Mapping/logs/spike_counting.o"
    include: sm.modules["spikes_counting"]
    expected_output.extend([__spikes_counting__output_json])

# Mark duplicates
if manager.config.mark_duplicates.do:
    __mark_duplicates__input = __bowtie2_mapping_ref__sort
    __mark_duplicates__output = "3-Deduplication/{{sample}}_{}_sort_dedup.bam".format(ref)
    __mark_duplicates__metrics = "3-Deduplication/{{sample}}_{}_sort_dedup.txt".format(ref)
    __mark_duplicates__log_std = "3-Deduplication/logs/{{sample}}_{}_sort_dedup.o".format(ref)
    __mark_duplicates__log_err = "3-Deduplication/logs/{{sample}}_{}_sort_dedup.e".format(ref)
    include: sm.modules["mark_duplicates"]
    expected_output.extend(expand(__mark_duplicates__output, sample=manager.samples))
else:
    __mark_duplicates__output = __bowtie2_mapping_ref__sort

# Remove blacklist
# if don't, use a .format() with a variable contains "_NoBlacklist" or not.
if manager.config.remove_blacklist.do:
    blacklist = "_NoBlacklist"
    blacklist_dir = "4-NoBlacklist"
    __remove_blacklist__input = __mark_duplicates__output
    __remove_blacklist__output = "4-NoBlacklist/{{sample}}_{}_sort_dedup{}.bam".format(ref, blacklist)
    __remove_blacklist__log_std = "4-NoBlacklist/logs/{{sample}}_{}_sort_dedup{}.o".format(ref, blacklist)
    __remove_blacklist__log_err = "4-NoBlacklist/logs/{{sample}}_{}_sort_dedup{}.e".format(ref, blacklist)
    include: sm.modules["remove_blacklist"]
    expected_output.extend(expand(__remove_blacklist__output, sample=manager.samples))

else:
    blacklist = ""
    blacklist_dir = "3-Deduplication"


# preIDR rules
# run preIDR on INPUT
if len(INPUT_REP) > 0:
    __preIDR_input__input_bam = expand("%s/{{INPUT}}_{REP}_%s_sort_dedup%s.bam" %
                                       (blacklist_dir,ref, blacklist), REP = rep)
    __preIDR_input__case = "Pool"
    __preIDR_input__log = "%s/logs/{{INPUT}}_preIDR_input.o" % (blacklist_dir)
    if len(rep) > 2:
        __preIDR_input__output = ["{}/{{INPUT}}_Pool_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist),
                                 "{}/{{INPUT}}_MaxiPool_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)]
    else:
        __preIDR_input__output = "{}/{{INPUT}}_Pool_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)
    include: preIDR_dynamic("input")
    expected_output.extend(expand(__preIDR_input__output, INPUT=INPUT_REP))


if len(IP_REP) > 0:
    # run SPR
    __preIDR_SPR__input_bam = expand("%s/{{IP}}_{REP}_%s_sort_dedup%s.bam" %
                                     (blacklist_dir,ref, blacklist) , REP = rep)
    __preIDR_SPR__case = "SPR"
    __preIDR_SPR__log = "%s/logs/{{IP}}_preIDR_SPR.o"% (blacklist_dir)
    __preIDR_SPR__output = expand("%s/{{IP}}_{SPR}_%s_sort_dedup%s.bam" % (blacklist_dir,ref, blacklist),  SPR = spr)
    include: preIDR_dynamic("SPR")
    expected_output.extend(expand(__preIDR_SPR__output, IP=IP_REP))

    # run PPR
    __preIDR_PPR__input_bam = expand("%s/{{IP}}_{REP}_%s_sort_dedup%s.bam" %
                                     (blacklist_dir,ref, blacklist), REP = rep)
    __preIDR_PPR__case = "PPR"
    __preIDR_PPR__log = "%s/logs/{IP}_preIDR_PPR.o" % (blacklist_dir)
    __preIDR_PPR__output =  expand("%s/{{IP}}_{PPR}_%s_sort_dedup%s.bam" %
                                   (blacklist_dir,ref, blacklist), PPR = ppr)
    include: preIDR_dynamic("PPR")
    expected_output.extend(expand(__preIDR_PPR__output, IP=IP_REP))

if manager.config.peak_calling.no_model:
    # PhantomPeakQualTools rule
    __spp__input = "{}/{{ALL}}_{}_sort_dedup{}.bam".format(blacklist_dir,ref, blacklist)
    __spp__output_pdf = "5-PhantomPeakQualTools/{{ALL}}_{}_sort_dedup{}_phantom.pdf".format(ref, blacklist)
    __spp__metrics = "5-PhantomPeakQualTools/{{ALL}}_{}_sort_dedup{}_spp.out".format(ref, blacklist)
    __spp__log_std = "5-PhantomPeakQualTools/logs/{ALL}_phantom.o"
    __spp__log_err = "5-PhantomPeakQualTools/logs/{ALL}_phantom.e"
    expected_output.extend(expand(__spp__metrics, ALL=ALL))
    include: sm.modules["spp"]
else :
    # PhantomPeakQualTools rule
    __spp__input = "{}/{{ALL}}_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)
    __spp__output_pdf = "5-PhantomPeakQualTools/{{ALL}}_{}_sort_dedup{}_phantom.pdf".format(ref, blacklist)
    __spp__metrics = "5-PhantomPeakQualTools/{{ALL}}_{}_sort_dedup{}_spp.out".format(ref, blacklist)
    __spp__log_std = "5-PhantomPeakQualTools/logs/{ALL}_phantom.o"
    __spp__log_err = "5-PhantomPeakQualTools/logs/{ALL}_phantom.e"
    expected_output.extend(expand(__spp__metrics, ALL=IP_ALL))
    include: sm.modules["spp"]

# Peak calling
model = manager.config.peak_calling.model_choice
if model in ["narrow", "broad"]:

    # Peak Calling on replicates
    if model in ["narrow"]:
        # add corresponding options
        __macs2_rep__options = "" + config["peak_calling"]['options']
    else:
        __macs2_rep__options = "--broad " + config["peak_calling"]['options']

    __macs2_rep__input_bam = "{}/{{IP_REP}}_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)
    __macs2_rep__input_done = ["{}/{{INPUT}}_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)]
    if manager.config.peak_calling.no_model:
        __macs2_rep__options += " --nomodel "
        __macs2_rep__input_done = [
            "5-PhantomPeakQualTools/{{IP_REP}}_{}_sort_dedup{}_spp.out".format(ref, blacklist)]
        __macs2_rep__shift_file = "5-PhantomPeakQualTools/{{IP_REP}}_{}_sort_dedup{}_spp.out".format(ref, blacklist)
    else:
        __macs2_rep__shift_file = "Empty"

    __macs2_rep__input = "-c {}/{{INPUT}}_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)
    __macs2_rep__log = "6-PeakCalling/{}/logs/{{IP_REP}}_vs_{{INPUT}}.o".format(model)
    __macs2_rep__output = "6-PeakCalling/{}/{{IP_REP}}_vs_{{INPUT}}_peaks.{}Peak".format(model, model)
    __macs2_rep__output_prefix = "6-PeakCalling/{}/{{IP_REP}}_vs_{{INPUT}}".format(model)
    expected_output.extend(expand(__macs2_rep__output, zip, IP_REP=IP_ALL, INPUT=INPUT))
    include: macs2_dynamic("rep")

    # Peak Calling on SPR
    if model in ["narrow"]:
        # add corresponding options
        __macs2_spr__options = "" + config["peak_calling"]['options']
    else:
        __macs2_spr__options = "--broad " + config["peak_calling"]['options']

    __macs2_spr__input_bam = "{}/{{SPR}}_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)
    __macs2_spr__input_done = ["{}/{{INPUT}}_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)]
    if manager.config.peak_calling.no_model:
        __macs2_spr__options += " --nomodel "
        __macs2_spr__input_done += ["5-PhantomPeakQualTools/{{SPR}}_{}_sort_dedup{}_spp.out".format(ref, blacklist)]
        __macs2_spr__shift_file = "5-PhantomPeakQualTools/{{SPR}}_{}_sort_dedup{}_spp.out".format(ref, blacklist)
    else:
        __macs2_spr__shift_file = "Empty"

    __macs2_spr__input = "-c {}/{{INPUT}}_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)
    __macs2_spr__log = "6-PeakCalling/{}/logs/{{SPR}}_vs_{{INPUT}}.o".format(model)
    __macs2_spr__output = "6-PeakCalling/{}/{{SPR}}_vs_{{INPUT}}_peaks.{}Peak".format(model, model)
    __macs2_spr__output_prefix = "6-PeakCalling/{}/{{SPR}}_vs_{{INPUT}}".format(model)
    expected_output.extend(expand(__macs2_spr__output, zip, SPR=IP_SPR, INPUT=INPUT_SPR))
    include: macs2_dynamic("spr")

    # Peak Calling on PPR
    if model in ["narrow"]:
        # add corresponding options
        __macs2_ppr__options = "" + config["peak_calling"]['options']
    else:
        __macs2_ppr__options = "--broad " + config["peak_calling"]['options']

    __macs2_ppr__input_bam = "{}/{{PPR}}_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)
    __macs2_ppr__input_done = ["{}/{{INPUT}}_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)]

    if manager.config.peak_calling.no_model:
        __macs2_ppr__options += " --nomodel "
        __macs2_ppr__input_done += ["5-PhantomPeakQualTools/{{PPR}}_{}_sort_dedup{}_spp.out".format(ref, blacklist)]
        __macs2_ppr__shift_file = "5-PhantomPeakQualTools/{{PPR}}_{}_sort_dedup{}_spp.out".format(ref, blacklist)
    else:
        __macs2_ppr__shift_file = "Empty"

    __macs2_ppr__input = "-c {}/{{INPUT}}_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)
    __macs2_ppr__log = "6-PeakCalling/{}/logs/{{PPR}}_vs_{{INPUT}}.o".format(model)
    __macs2_ppr__output = "6-PeakCalling/{}/{{PPR}}_vs_{{INPUT}}_peaks.{}Peak".format(model, model)
    __macs2_ppr__output_prefix = "6-PeakCalling/{}/{{PPR}}_vs_{{INPUT}}".format(model)
    expected_output.extend(expand(__macs2_ppr__output, zip, PPR=IP_PPR, INPUT=INPUT_PPR))
    include: macs2_dynamic("ppr")

    # Peak Calling on Pool files
    if model in ["narrow"]:
        # add corresponding options
        __macs2_pool__options = "" + config["peak_calling"]['options']
    else:
        __macs2_pool__options = "--broad " + config["peak_calling"]['options']

    __macs2_pool__input_bam = "{}/{{POOL}}_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)
    __macs2_pool__input_done = ["{}/{{INPUT_POOL}}_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)]

    if manager.config.peak_calling.no_model:
        __macs2_pool__options += " --nomodel "
        __macs2_pool__input_done += ["5-PhantomPeakQualTools/{{POOL}}_{}_sort_dedup{}_spp.out".format(ref, blacklist)]
        __macs2_pool__shift_file = "5-PhantomPeakQualTools/{{POOL}}_{}_sort_dedup{}_spp.out".format(ref, blacklist)
    else:
        __macs2_pool__shift_file = "Empty"

    __macs2_pool__input = "-c {}/{{INPUT_POOL}}_{}_sort_dedup{}.bam".format(blacklist_dir, ref, blacklist)
    __macs2_pool__log = "6-PeakCalling/{}/logs/{{POOL}}_vs_{{INPUT_POOL}}.o".format(model)
    __macs2_pool__output = "6-PeakCalling/{}/{{POOL}}_vs_{{INPUT_POOL}}_peaks.{}Peak".format(model, model)
    __macs2_pool__output_prefix = "6-PeakCalling/{}/{{POOL}}_vs_{{INPUT_POOL}}".format(model)
    expected_output.extend(expand(__macs2_pool__output, zip, POOL=PPR_POOL, INPUT_POOL=INPUT_POOL))
    include: macs2_dynamic("pool")


else:
    raise ValueError("Invalid choice of model for peak calling. Use either narrow or broad, or set no-model to yes")

# Compute IDR
__compute_idr__input1 = "6-PeakCalling/{}/{{IP_IDR}}_{{CASE}}1_vs_{{INPUT}}_peaks.{}Peak".format(model, model)
__compute_idr__input2 = "6-PeakCalling/{}/{{IP_IDR}}_{{CASE}}2_vs_{{INPUT}}_peaks.{}Peak".format(model, model)
__compute_idr__output = "7-IDR/{{IP_IDR}}_{{CASE}}1vs{{CASE}}2_{{INPUT}}_ref_{}_idr.txt".format(model)
__compute_idr__output_peak = "7-IDR/{{IP_IDR}}_{{CASE}}1vs{{CASE}}2_{{INPUT}}_ref_{}_idr{}.{}Peak".format(
    model, config["compute_idr"]["thresh"], model)
__compute_idr__log = "7-IDR/logs/{IP_IDR}_{CASE}1vs{CASE}2_%s_idr.o" % model
include: sm.modules["compute_idr"]
expected_output.extend(expand(__compute_idr__output, zip, IP_IDR=REP_IDR, CASE=CASE, INPUT=IN_IDR))


# Select IDR peaks
__select_peaks__input_rep = "7-IDR/{{IP_IDR}}_{}1vs{}2_{{INPUT}}_ref_{}_idr{}.{}Peak".format(rep_flag, rep_flag, model, config["compute_idr"]["thresh"], model)
__select_peaks__input_ppr = "7-IDR/{{IP_IDR}}_{}1vs{}2_{{INPUT}}_ref_{}_idr{}.{}Peak".format("PPR", "PPR", model, config["compute_idr"]["thresh"], model)
__select_peaks__input_pool = "6-PeakCalling/{}/{{IP_IDR}}_PPRPool_vs_{{INPUT}}_peaks.{}Peak".format(model, model)
__select_peaks__logs = "7-IDR/logs/{IP_IDR}_selected_peaks.o"
__select_peaks__output = "7-IDR/{{IP_IDR}}_vs_{{INPUT}}_select.{}Peak".format(model)
include: sm.modules["select_peaks"]
expected_output.extend(expand(__select_peaks__output, zip, IP_IDR=IP_REP, INPUT=IP_REP_DUP))




#Compute peak metrics
idr_peaks = []
idr_peaks.extend(expand("7-IDR/{{IP_IDR}}_{{CASE}}1vs{{CASE}}2_{{INPUT}}_ref_{}_idr{}.{}Peak".format(
                        model, config["compute_idr"]["thresh"], model), zip, IP_IDR=REP_IDR, CASE=CASE, INPUT=IN_IDR))
__metrics_peaks__input = idr_peaks
__metrics_peaks__marks = marks
__metrics_peaks__conds = conds
__metrics_peaks__rep = rep_flag
__metrics_peaks__logs = "7-IDR/logs/peak_metrics.out"
__metrics_peaks__output = "metrics_peaks_mqc.out"
include: sm.modules["metrics_peaks"]
expected_output.extend([__metrics_peaks__output])


def getPeakFilesByMark(wildcards):
    ALL_IP = expand(__select_peaks__output, zip, IP_IDR=IP_REP, INPUT=IP_REP_DUP)
    IP_dict = {}
    for mark in marks:
        IP_dict[mark] = []
        for file in ALL_IP:
            if mark in file:
                IP_dict[mark].append(file)

    return IP_dict[wildcards["MARK"]]



# get union from all detected peak among all conditions for each mark
# TODO test with 3 conditions using a different way to call all cond in the output name
__union_peaks__input = getPeakFilesByMark
__union_peaks__logs = "7-IDR/logs/{MARK}_Optimal_selected_peaks.o"
__union_peaks__output = "7-IDR/{{MARK}}_{}u{}_{}.optimal.{}Peak_overlap.bed".format(conds[0], conds[1], ref, model)
include: sm.modules["union_peaks"]
expected_output.extend(expand(__union_peaks__output, MARK=marks))


# get GFF from peak files
__bed_to_gff__input = "7-IDR/{{MARK}}_{}u{}_{}.optimal.{}Peak_overlap.bed".format(conds[0], conds[1], ref, model)
__bed_to_gff__logs = "7-IDR/logs/{MARK}_Optimal_bed2gff.o"
__bed_to_gff__output = "7-IDR/{{MARK}}_{}u{}_{}.optimal.{}Peak_overlap.gff".format(conds[0], conds[1], ref, model)
include: sm.modules["bed_to_gff"]
expected_output.extend(expand(__bed_to_gff__output,  MARK=marks))


def getBAMFilesByMark(wildcards):
    ALL_BAM = expand(__macs2_rep__input_bam, IP_REP=IP_ALL)
    BAM_dict = {}
    for mark in marks:
        BAM_dict[mark] = []
        for file in ALL_BAM:
            if mark in file:
                BAM_dict[mark].append(file)

    return BAM_dict[wildcards["MARK"]]


# feature Count on peaks
__feature_counts__input = getBAMFilesByMark
__feature_counts__output_count = "7-IDR/{{MARK}}_Matrix_Optimal_{}Peak.mtx".format(model)
__feature_counts__gff = "7-IDR/{{MARK}}_{}u{}_{}.optimal.{}Peak_overlap.gff".format(conds[0], conds[1], ref, model)
__feature_counts__log = "6-PeakCalling/%s/logs/{MARK}_counts.o" % model
__feature_counts__options = "-t peak -g gene_id"
__feature_counts__threads = 4
include: sm.modules["feature_counts"]
expected_output.extend(expand(__feature_counts__output_count,  MARK=marks))



# !Reset expected_output variable after multiqc
# Multiqc rule
__multiqc__input = expected_output
__multiqc__input_dir = "."
__multiqc__logs = "multiqc/multiqc.log"
__multiqc__output = config['multiqc']['output-directory'] + "/multiqc_report.html"
#include: sm.modules["multiqc"]
#expected_output = [__multiqc__output]

# Include rule graph for each sample
__rulegraph__input = __snakefile__
__rulegraph__output = "rulegraph/rulegraph.svg"
__rulegraph__mapper = {"fastqc_samples": "fastqc_samples/"}
include: sm.modules['rulegraph']
expected_output.extend([__rulegraph__output])

# Add Conda
__conda__output = "requirements.txt"
include: sm.modules['conda']  # Create requirements.txt(dependencies)
expected_output.extend([__conda__output])

# Those rules takes a couple of seconds so no need for a cluster
localrules: conda, rulegraph

rule chipseq:
    input: expected_output

onsuccess:
    import os
    # Create plots about stats
    sm.plot_stats(N=len(manager.samples))

    try: os.mkdir("cluster_logs")
    except:pass

    try: shell("mv slurm* cluster_logs/")
    except: pass

    #create a file which suppress all PPR, SPR, and deduplicates/blacklist and trimmed files
    sm.OnSuccess()() # create instance to create main cleanup


onerror:
    print("An error occurred. See message above.")
